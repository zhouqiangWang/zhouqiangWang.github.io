[{"title":"III","slug":null,"date":"2017-09-11T18:01:19.000Z","updated":null,"comments":null,"path":"2017/09/12/III/","link":null,"permalink":null,"excerpt":null,"keywords":null,"text":"Video #7: Selecting the best model using cross-validationthe drawback of using the train/test split procedure for model evaluation high variance estimate since changing which observations happen to be in the testing set can significantly change testing accuracy Steps for K-fold cross-validation Split the dataset into K equal partitions (or “folds”). Use fold 1 as the testing set and the union of the other folds as the training set. Calculate testing accuracy. Repeat steps 2 and 3 K times, using a different fold as the testing set each time. Use the average testing accuracy as the estimate of out-of-sample accuracy. Comparing cross-validation to train/test splitAdvantages of cross-validation: More accurate estimate of out-of-sample accuracy More “efficient” use of data (every observation is used for both training and testing) Advantages of train/test split: Runs K times faster than K-fold cross-validation Simpler to examine the detailed results of the testing process Cross-validation recommendations K can be any number, but K=10 is generally recommended For classification problems, stratified sampling is recommended for creating the folds Each response class should be represented with equal proportions in each of the K folds scikit-learn’s cross_val_score function does this by default Cross-validation example: parameter tuningGoal: Select the best tuning parameters (aka “hyperparameters”) for KNN on the iris dataset Cross-validation example: model selectionGoal: Compare the best KNN model with logistic regression on the iris dataset Cross-validation example: feature selectionGoal: Select whether the Newspaper feature should be included in the linear regression model on the advertising dataset Improvements to cross-validation Repeated cross-validation Creating a hold-out set Feature engineering and selection within cross-validation iterations Video #8: Efficiently searching for optimal tuning parametersMore efficient parameter tuning using GridSearchCVAllows you to define a grid of parameters that will be searched using K-fold cross-validation Searching multiple parameters simultaneously Example: tuning max_depth and min_samples_leaf for a DecisionTreeClassifier Could tune parameters independently: change max_depth while leaving min_samples_leaf at its default value, and vice versa But, best performance might be achieved when neither parameter is at its default value Using the best parameters to make predictionsReducing computational expense using RandomizedSearchCV Searching many different parameters at once may be computationally infeasible RandomizedSearchCV searches a subset of the parameters, and you control the computational “budget” Important: Specify a continuous distribution (rather than a list of values) for any continous parameters Video #9: Better evaluation of classification modelsModel evaluation procedures Training and testing on the same data Rewards overly complex models that “overfit” the training data and won’t necessarily generalize Train/test split Split the dataset into two pieces, so that the model can be trained and tested on different data Better estimate of out-of-sample performance, but still a “high variance” estimate Useful due to its speed, simplicity, and flexibility K-fold cross-validation Systematically create “K” train/test splits and average the results together Even better estimate of out-of-sample performance Runs “K” times slower than train/test split Model evaluation metrics Regression problems: Mean Absolute Error, Mean Squared Error, Root Mean Squared Error Classification problems: Classification accuracy Classification accuracy Classification accuracy: percentage of correct Null accuracy: accuracy that could be achieved by always predicting the most frequent class Conclusion: Classification accuracy is the easiest classification metric to understand But, it does not tell you the underlying distribution of response values And, it does not tell you what “types” of errors your classifier is making Confusion matrixTable that describes the performance of a classification model Basic terminology True Positives (TP): we correctly predicted that they do have diabetes True Negatives (TN): we correctly predicted that they don’t have diabetes False Positives (FP): we incorrectly predicted that they do have diabetes (a “Type I error”) False Negatives (FN): we incorrectly predicted that they don’t have diabetes (a “Type II error”) Adjusting the classification thresholdDecrease the threshold for predicting diabetes in order to increase the sensitivity of the classifier Sensitivity: When the actual value is positive, how often is the prediction correct? How “sensitive” is the classifier to detecting positive instances?Also known as “True Positive Rate” or “Recall” Specificity: When the actual value is negative, how often is the prediction correct? How “specific” (or “selective”) is the classifier in predicting positive instances? Conclusion: Threshold of 0.5 is used by default (for binary problems) to convert predicted probabilities into class predictions Threshold can be adjusted to increase sensitivity or specificity Sensitivity and specificity have an inverse relationship ROC Curves and Area Under the Curve (AUC)Question: Wouldn’t it be nice if we could see how sensitivity and specificity are affected by various thresholds, without actually changing the threshold?Answer: Plot the ROC curve! AUC is the percentage of the ROC plot that is underneath the curve: AUC is useful as a single number summary of classifier performance. If you randomly chose one positive and one negative observation, AUC represents the likelihood that your classifier will assign a higher predicted probability to the positive observation. AUC is useful even when there is high class imbalance (unlike classification accuracy). Confusion matrix advantages: Allows you to calculate a variety of metrics Useful for multi-class problems (more than two response classes) ROC/AUC advantages: Does not require you to set a classification threshold Still useful when there is high class imbalance (function(d, s) { var j, e = d.getElementsByTagName(s)[0]; if (typeof LivereTower === &apos;function&apos;) { return; } j = d.createElement(s); j.src = &apos;https://cdn-city.livere.com/js/embed.dist.js&apos;; j.async = true; e.parentNode.insertBefore(j, e); })(document, ‘script’); 为正常使用来必力评论功能请激活JavaScript","raw":null,"content":null,"categories":null,"tags":[]},{"title":"model","slug":null,"date":"2017-09-09T11:50:09.000Z","updated":null,"comments":null,"path":"2017/09/09/model/","link":null,"permalink":null,"excerpt":null,"keywords":null,"text":"video 4. Model training and prediction4-step modeling pattern Import the class you plan to use; Instantiate the estimator; Instantiate : make a instance of Estimator : model Fit the model with data(aka “model training”); Predict the response for a new observation Using a different value for KUsing a different classification model video 5. Comparing machine learning modelsEvaluation procedure #1: Train and test on the entire datasetEvaluation procedure #2: Train/test split Split the dataset into two pieces: a training set and a testing set. Train the model on the training set. Test the model on the testing set, and evaluate how well we did.20%~40% of dataset are split as test data. Video #6: Data science pipeline with pandas, seaborn, scikit-learnTypes of supervised learning Classification: Predict a categorical response Regression: Predict a continuous response Reading data using pandas1import pandas as pd 12345# read CSV file directly from a URL and save the resultsdata = pd.read_csv(&apos;./train.csv&apos;, index_col=0)# display the first 5 rowsdata.head() 12# display the last 5 rowsdata.tail() 1data.shape Visualizing data using seabornSeaborn: Python library for statistical data visualization built on top of Matplotlib 1234import seaborn as sb# allow plots to appear within the notebook%matplotlib inline 12# visualize the relationship between the features and the response using scatterplotssns.pairplot(data, x_vars=[&apos;TV&apos;,&apos;Radio&apos;,&apos;Newspaper&apos;], y_vars=&apos;Sales&apos;, size=7, aspect=0.7, kind=&apos;reg&apos;) Linear regressionPros: fast, no tuning required, highly interpretable, well-understood Cons: unlikely to produce the best predictive accuracy (presumes a linear relationship between the features and response) Preparing X and y using pandas Splitting X and y into training and testing sets Linear regression in scikit-learn Interpreting model coefficients Making predictions Model evaluation metrics for regression Mean Absolute Error (MAE) is the mean of the absolute value of the errors: Mean Squared Error (MSE) is the mean of the squared errors: Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors: Comparing these metrics: MAE is the easiest to understand, because it’s the average error. MSE is more popular than MAE, because MSE “punishes” larger errors. RMSE is even more popular than MSE, because RMSE is interpretable in the “y” units. Feature selectionremove some feature from the model and check the RMSE!","raw":null,"content":null,"categories":null,"tags":[]},{"title":"Machine Learning scikit","slug":null,"date":"2017-09-03T09:38:26.000Z","updated":null,"comments":null,"path":"2017/09/03/Machine-Learning-scikit/","link":null,"permalink":null,"excerpt":null,"keywords":null,"text":"video 1. Intro to Machine Learning. What is machine learningsemi-automated extraction of knowledge from data. two main categories of machine learning? supervised - predictive modeling, making predictions using data; unsupervised learning - extract structure from data or learning how to best represent data. two main steps of supervised learning: train a machine learning model using existing labeled data. make prediction on new coming data. video 2. set-up python environment for machine learning video 3. Exploring the Iris datasetMachine learning terminology observation : Each row in dataset(also known as: sample, example, instance, record); feature : Each column in dataset(aka. predictor, attribute, independent variable, input, regressor, covariate); response : Each value we are predicting(aka: target, outcome, label, dependent variable); Classification : supervised learning in which the response is categorical; Regression : supervised learning in which the response is ordered and continuous. Requirements for working with data in scikit-learn Features and response are separate objects; Features and response should be numeric; Features and response should be NumPy arrays; Features and response should have specific shapes.","raw":null,"content":null,"categories":null,"tags":[]},{"title":"Hello World","slug":null,"date":"2016-05-23T20:12:38.000Z","updated":null,"comments":null,"path":"2016/05/24/hello-world/","link":null,"permalink":null,"excerpt":null,"keywords":null,"text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment (function(d, s) { var j, e = d.getElementsByTagName(s)[0]; if (typeof LivereTower === &apos;function&apos;) { return; } j = d.createElement(s); j.src = &apos;https://cdn-city.livere.com/js/embed.dist.js&apos;; j.async = true; e.parentNode.insertBefore(j, e); })(document, ‘script’); 为正常使用来必力评论功能请激活JavaScript","raw":null,"content":null,"categories":null,"tags":[]}]